{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONt0yxP/TxIIhRhF+i1i4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prafuldash/MyScala_Work/blob/master/Mypythonwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P31S2LlObXSY",
        "outputId": "4c627d06-72b5-4f70-8abc-0882b873ff30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello PRAFUL\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello PRAFUL\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class employee():\n",
        "\n",
        "    def __init__(self,name,age,id,salary):\n",
        "     self.name=name\n",
        "     self.id=id\n",
        "     self.age=age\n",
        "     self.salary=salary\n",
        "\n",
        "emp1=(\"PRAFUL\",33,3333,2234)\n",
        "emp2=(\"PRAddFUL\",3333,4433,1234)\n",
        "print(emp1,emp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKlkUrhKrTlx",
        "outputId": "e3aa1391-3600-4e33-d207-bdb7151151cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('PRAFUL', 33, 3333, 2234) ('PRAddFUL', 3333, 4433, 1234)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class car:\n",
        "  def __init__(self,modelname,year):\n",
        "    self.modelname=modelname\n",
        "    self.year=year\n",
        "\n",
        "  def display(self):\n",
        "      print(self.modelname,self.year)\n",
        "\n",
        "c1=car(\"Toyota\",2022)\n",
        "c1.display()\n",
        "#print(c1)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clrueZxYuLDI",
        "outputId": "afaaf8ae-7a94-4190-9cc4-4c4ebd80cc40"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toyota 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f= open('file.txt','r')\n",
        "f.read(3)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "ZLAr81oj3CA1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=open('file.txt','a')\n",
        "a.write(\"I m PRAFUL KUMAR DASH FROM INDIA\")\n",
        "a.close()"
      ],
      "metadata": {
        "id": "qCTysts23ywW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=open('file.txt','r')\n",
        "for x in a:\n",
        " print(x)\n",
        "a=open('file.txt','r')\n",
        "for x in a:\n",
        " print(x)\n",
        "a.close()a.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BmKEc4ml4agT",
        "outputId": "9197c3b4-8a98-4e11-b9c2-3c42dabccab6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "sudo hadoop fs -mkdir /user/hduser\n",
            "\n",
            "hadoop fs -copyFromLocal test.txt /user/spark\n",
            "\n",
            "bin/rbe-env.sh shutdown\n",
            "\n",
            "hadoop fs -ls /user\n",
            "\n",
            "sudo bash\n",
            "\n",
            " su hdfs\n",
            "\n",
            "  hadoop fs -mkdir /user/hduser\n",
            "\n",
            "  hadoop fs -chown -R hduser:supergroup /user/hduser\n",
            "\n",
            "  \n",
            "\n",
            "  http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.1.7/bk_installing_manually_book/content/rpm-chap-hue-2.html\n",
            "\n",
            "  \n",
            "\n",
            "  http://gethue.com/hadoop-hue-3-on-hdp-installation-tutorial/\n",
            "\n",
            "  http://gethue.com/how-to-deploy-hue-on-hdp/\n",
            "\n",
            "updates\n",
            "\n",
            "insert\n",
            "\n",
            "delete\n",
            "\n",
            "\n",
            "\n",
            "INSERT INTO TABLE students VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32); \n",
            "\n",
            "\n",
            "\n",
            "UPDATE students SET name='PRAFUL', age='28' WHERE name='Alfreds Futterkiste';\n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE students (name VARCHAR(64), age INT, gpa DECIMAL(3, 2))\n",
            "\n",
            "  CLUSTERED BY (age) INTO 2 BUCKETS STORED AS ORC;\n",
            "\n",
            "\n",
            "\n",
            "13016288\n",
            "\n",
            "\n",
            "\n",
            "set hive.execution.engine=tez;\n",
            "\n",
            "set hive.execution.engine=mr;\n",
            "\n",
            "set hive.enable.mrr=true;\n",
            "\n",
            "set hive.use.tez.natively=true;\n",
            "\n",
            "\n",
            "\n",
            "--http://hadooptutorial.info/hive-database-commands/\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--SELECT current_database() //(as of Hive 0.13.0)\n",
            "\n",
            "\n",
            "\n",
            "Hive data types\n",
            "\n",
            "BIGINT Data Type\n",
            "\n",
            "BOOLEAN Data Type\n",
            "\n",
            "CHAR Data Type (CDH 5.2 or higher only)\n",
            "\n",
            "DECIMAL Data Type (CDH 5.1 or higher only)\n",
            "\n",
            "DOUBLE Data Type\n",
            "\n",
            "FLOAT Data Type\n",
            "\n",
            "INT Data Type\n",
            "\n",
            "REAL Data Type\n",
            "\n",
            "SMALLINT Data Type\n",
            "\n",
            "STRING Data Type\n",
            "\n",
            "TIMESTAMP Data Type\n",
            "\n",
            "TINYINT Data Type\n",
            "\n",
            "VARCHAR Data Type (CDH 5.2 or higher only)\n",
            "\n",
            "\n",
            "\n",
            "1. Numeric Types\n",
            "\n",
            "TINYINT (1-byte signed integer, from -128 to 127)\n",
            "\n",
            "SMALLINT (2-byte signed integer, from -32,768 to 32,767)\n",
            "\n",
            "INT (4-byte signed integer, from -2,147,483,648 to 2,147,483,647)\n",
            "\n",
            "BIGINT (8-byte signed integer, from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807)\n",
            "\n",
            "FLOAT (4-byte single precision floating point number)\n",
            "\n",
            "DOUBLE (8-byte double precision floating point number)\n",
            "\n",
            "DECIMAL (Hive 0.13.0 introduced user definable precision and scale)\n",
            "\n",
            "2. Date/Time Types\n",
            "\n",
            "TIMESTAMP\n",
            "\n",
            "DATE\n",
            "\n",
            "3. String Types\n",
            "\n",
            "STRING\n",
            "\n",
            "VARCHAR\n",
            "\n",
            "CHAR\n",
            "\n",
            "4. Misc Types\n",
            "\n",
            "BOOLEAN\n",
            "\n",
            "BINARY\n",
            "\n",
            "Apart from these primitive data types Hive offers some complex data types which are listed below:\n",
            "\n",
            "\n",
            "\n",
            "5. Complex Types\n",
            "\n",
            "arrays: ARRAY<data_type>\n",
            "\n",
            "maps: MAP<primitive_type, data_type>\n",
            "\n",
            "structs: STRUCT<col_name : data_type [COMMENT col_comment], ...>\n",
            "\n",
            "union: UNIONTYPE<data_type, data_type, ...>\n",
            "\n",
            "\n",
            "\n",
            "******hive******\n",
            "\n",
            ">org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "http://datametica.com/introduction-to-hive-transactions/\n",
            "\n",
            "\n",
            "\n",
            "http://pivotalhd.docs.pivotal.io/docs/hive-users-guide.html\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "create table praful(id int, id2 string) \n",
            "\n",
            "clustered by (id) into 8 buckets \n",
            "\n",
            "stored as orc TBLPROPERTIES ('transactional'='true'); \n",
            "\n",
            "insert into table PRAFUL values(1,'abc');\n",
            "\n",
            "UPDATE praful SET id2='praful' WHERE id=1;\n",
            "\n",
            "\n",
            "\n",
            "delete from praful where id=1;\n",
            "\n",
            "\n",
            "\n",
            "hive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "172.19.36.143---https://cwiki.apache.org/confluence/display/Hive/Tutorial\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "https://cwiki.apache.org/confluence/display/Hive/Tutorial\n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE page_view(viewTime INT, userid BIGINT,\n",
            "\n",
            "                page_url STRING, referrer_url STRING,\n",
            "\n",
            "                ip STRING COMMENT 'IP Address of the User')\n",
            "\n",
            "COMMENT 'This is the page view table'\n",
            "\n",
            "PARTITIONED BY(dt STRING, country STRING)\n",
            "\n",
            "STORED AS SEQUENCEFILE;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE page_view(viewTime INT, userid BIGINT,\n",
            "\n",
            "                page_url STRING, referrer_url STRING,\n",
            "\n",
            "                ip STRING COMMENT 'IP Address of the User')\n",
            "\n",
            "COMMENT 'This is the page view table'\n",
            "\n",
            "PARTITIONED BY(dt STRING, country STRING)\n",
            "\n",
            "ROW FORMAT DELIMITED\n",
            "\n",
            "        FIELDS TERMINATED BY '1'\n",
            "\n",
            "STORED AS SEQUENCEFILE;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE page_view(viewTime INT, userid BIGINT,\n",
            "\n",
            "                page_url STRING, referrer_url STRING,\n",
            "\n",
            "                ip STRING COMMENT 'IP Address of the User')\n",
            "\n",
            "COMMENT 'This is the page view table'\n",
            "\n",
            "PARTITIONED BY(dt STRING, country STRING)\n",
            "\n",
            "CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n",
            "\n",
            "ROW FORMAT DELIMITED\n",
            "\n",
            "        FIELDS TERMINATED BY '1'\n",
            "\n",
            "        COLLECTION ITEMS TERMINATED BY '2'\n",
            "\n",
            "        MAP KEYS TERMINATED BY '3'\n",
            "\n",
            "STORED AS SEQUENCEFILE;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE page_view(viewTime INT, userid BIGINT,\n",
            "\n",
            "                page_url STRING, referrer_url STRING,\n",
            "\n",
            "                friends ARRAY<BIGINT>, properties MAP<STRING, STRING>\n",
            "\n",
            "                ip STRING COMMENT 'IP Address of the User')\n",
            "\n",
            "COMMENT 'This is the page view table'\n",
            "\n",
            "PARTITIONED BY(dt STRING, country STRING)\n",
            "\n",
            "CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n",
            "\n",
            "ROW FORMAT DELIMITED\n",
            "\n",
            "        FIELDS TERMINATED BY '1'\n",
            "\n",
            "        COLLECTION ITEMS TERMINATED BY '2'\n",
            "\n",
            "        MAP KEYS TERMINATED BY '3'\n",
            "\n",
            "STORED AS SEQUENCEFILE;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SHOW TABLES;\n",
            "\n",
            "SHOW TABLES 'page.*';\n",
            "\n",
            "SHOW PARTITIONS page_view;\n",
            "\n",
            "DESCRIBE page_view;\n",
            "\n",
            "DESCRIBE EXTENDED page_view;\n",
            "\n",
            "DESCRIBE EXTENDED page_view PARTITION (ds='2008-08-08');\n",
            "\n",
            "ALTER TABLE old_table_name RENAME TO new_table_name;\n",
            "\n",
            "ALTER TABLE old_table_name REPLACE COLUMNS (col1 TYPE, ...);\n",
            "\n",
            "ALTER TABLE tab1 ADD COLUMNS (c1 INT COMMENT 'a new int column', c2 STRING DEFAULT 'def val');\n",
            "\n",
            "DROP TABLE pv_users;\n",
            "\n",
            "ALTER TABLE pv_users DROP PARTITION (ds='2008-08-08')\n",
            "\n",
            "\n",
            "\n",
            "Loading Data-----\n",
            "\n",
            "\n",
            "\n",
            "CREATE EXTERNAL TABLE page_view_stg(viewTime INT, userid BIGINT,\n",
            "\n",
            "                page_url STRING, referrer_url STRING,\n",
            "\n",
            "                ip STRING COMMENT 'IP Address of the User',\n",
            "\n",
            "                country STRING COMMENT 'country of origination')\n",
            "\n",
            "COMMENT 'This is the staging page view table'\n",
            "\n",
            "ROW FORMAT DELIMITED FIELDS TERMINATED BY '44' LINES TERMINATED BY '12'\n",
            "\n",
            "STORED AS TEXTFILE\n",
            "\n",
            "LOCATION '/user/data/staging/page_view';\n",
            "\n",
            " \n",
            "\n",
            "hadoop dfs -put /tmp/pv_2008-06-08.txt /user/data/staging/page_view\n",
            "\n",
            " \n",
            "\n",
            "FROM page_view_stg pvs\n",
            "\n",
            "INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='US')\n",
            "\n",
            "SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip\n",
            "\n",
            "WHERE pvs.country = 'US';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LOAD DATA LOCAL INPATH /tmp/pv_2008-06-08_us.txt INTO TABLE page_view PARTITION(date='2008-06-08', country='US')\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LOAD DATA INPATH '/user/data/pv_2008-06-08_us.txt' INTO TABLE page_view PARTITION(date='2008-06-08', country='US')\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE TABLE user_active\n",
            "\n",
            "SELECT user.*\n",
            "\n",
            "FROM user\n",
            "\n",
            "WHERE user.active = 1;\n",
            "\n",
            "\n",
            "\n",
            "SELECT user.*\n",
            "\n",
            "FROM user\n",
            "\n",
            "WHERE user.active = 1;\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE TABLE xyz_com_page_views\n",
            "\n",
            "SELECT page_views.*\n",
            "\n",
            "FROM page_views\n",
            "\n",
            "WHERE page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31' AND\n",
            "\n",
            "      page_views.referrer_url like '%xyz.com';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_users\n",
            "\n",
            "SELECT pv.*, u.gender, u.age\n",
            "\n",
            "FROM user u JOIN page_view pv ON (pv.userid = u.id)\n",
            "\n",
            "WHERE pv.date = '2008-03-03';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_users\n",
            "\n",
            "SELECT pv.*, u.gender, u.age\n",
            "\n",
            "FROM user u FULL OUTER JOIN page_view pv ON (pv.userid = u.id)\n",
            "\n",
            "WHERE pv.date = '2008-03-03';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In order check the existence of a key in another table, the user can use LEFT SEMI JOIN as illustrated by the following example.\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_users\n",
            "\n",
            "SELECT u.*\n",
            "\n",
            "FROM user u LEFT SEMI JOIN page_view pv ON (pv.userid = u.id)\n",
            "\n",
            "WHERE pv.date = '2008-03-03';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In order to join more than one tables, the user can use the following syntax:\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_friends\n",
            "\n",
            "SELECT pv.*, u.gender, u.age, f.friends\n",
            "\n",
            "FROM page_view pv JOIN user u ON (pv.userid = u.id) JOIN friend_list f ON (u.id = f.uid)\n",
            "\n",
            "WHERE pv.date = '2008-03-03';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In order to count the number of distinct users by gender one could write the following query:\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_gender_sum\n",
            "\n",
            "SELECT pv_users.gender, count (DISTINCT pv_users.userid)\n",
            "\n",
            "FROM pv_users\n",
            "\n",
            "GROUP BY pv_users.gender;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Multiple aggregations can be done at the same time, however, no two aggregations can have different DISTINCT columns .e.g while the following \n",
            "\n",
            "\n",
            "\n",
            "is possible\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_gender_agg\n",
            "\n",
            "SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid)\n",
            "\n",
            "FROM pv_users\n",
            "\n",
            "GROUP BY pv_users.gender;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "however, the following query is not allowed\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_gender_agg\n",
            "\n",
            "SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip)\n",
            "\n",
            "FROM pv_users\n",
            "\n",
            "GROUP BY pv_users.gender;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FROM pv_users\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_gender_sum\n",
            "\n",
            "    SELECT pv_users.gender, count_distinct(pv_users.userid)\n",
            "\n",
            "    GROUP BY pv_users.gender\n",
            "\n",
            " \n",
            "\n",
            "INSERT OVERWRITE DIRECTORY '/user/data/tmp/pv_age_sum'\n",
            "\n",
            "    SELECT pv_users.age, count_distinct(pv_users.userid)\n",
            "\n",
            "    GROUP BY pv_users.age;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FROM page_view_stg pvs\n",
            "\n",
            "INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='US')\n",
            "\n",
            "       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'US'\n",
            "\n",
            "INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='CA')\n",
            "\n",
            "       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'CA'\n",
            "\n",
            "INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='UK')\n",
            "\n",
            "       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'UK';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FROM page_view_stg pvs\n",
            "\n",
            "INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country)\n",
            "\n",
            "       SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.country\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "set hive.exec.dynamic.partition.mode=nonstrict;\n",
            "\n",
            "    beeline> FROM page_view_stg pvs\n",
            "\n",
            "          INSERT OVERWRITE TABLE page_view PARTITION(dt, country)\n",
            "\n",
            "                 SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\n",
            "\n",
            "                        from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country;\n",
            "\n",
            "...\n",
            "\n",
            "2010-05-07 11:10:19,816 Stage-1 map = 0%,  reduce = 0%\n",
            "\n",
            "[Fatal Error] Operator FS_28 (id=41): fatal error. Killing the job.\n",
            "\n",
            "Ended Job = job_201005052204_28178 with errors\n",
            "\n",
            "...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "set hive.exec.dynamic.partition.mode=nonstrict;\n",
            "\n",
            "beeline> FROM page_view_stg pvs\n",
            "\n",
            "      INSERT OVERWRITE TABLE page_view PARTITION(dt, country)\n",
            "\n",
            "             SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip,\n",
            "\n",
            "                    from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country\n",
            "\n",
            "             DISTRIBUTE BY ds, country;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE LOCAL DIRECTORY '/tmp/pv_gender_sum'\n",
            "\n",
            "SELECT pv_gender_sum.*\n",
            "\n",
            "FROM pv_gender_sum;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE TABLE pv_gender_sum_sample\n",
            "\n",
            "SELECT pv_gender_sum.*\n",
            "\n",
            "FROM pv_gender_sum TABLESAMPLE(BUCKET 3 OUT OF 32);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TABLESAMPLE(BUCKET x OUT OF y)\n",
            "\n",
            "\n",
            "\n",
            "TABLESAMPLE(BUCKET 3 OUT OF 16)\n",
            "\n",
            "\n",
            "\n",
            "TABLESAMPLE(BUCKET 3 OUT OF 64 ON userid)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE TABLE actions_users\n",
            "\n",
            "SELECT u.id, actions.date\n",
            "\n",
            "FROM (\n",
            "\n",
            "    SELECT av.uid AS uid\n",
            "\n",
            "    FROM action_video av\n",
            "\n",
            "    WHERE av.date = '2008-06-03'\n",
            "\n",
            " \n",
            "\n",
            "    UNION ALL\n",
            "\n",
            " \n",
            "\n",
            "    SELECT ac.uid AS uid\n",
            "\n",
            "    FROM action_comment ac\n",
            "\n",
            "    WHERE ac.date = '2008-06-03'\n",
            "\n",
            "    ) actions JOIN users u ON(u.id = actions.uid); \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE array_table (int_array_column ARRAY<INT>);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SELECT pv.friends[2]\n",
            "\n",
            "FROM page_views pv;\n",
            "\n",
            "SELECT pv.userid, size(pv.friends)\n",
            "\n",
            "FROM page_view pv;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SELECT pv.userid, size(pv.friends)\n",
            "\n",
            "FROM page_view pv;\n",
            "\n",
            "\n",
            "\n",
            "INSERT OVERWRITE page_views_map\n",
            "\n",
            "SELECT pv.userid, pv.properties['page type']\n",
            "\n",
            "FROM page_views pv;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SELECT size(pv.properties)\n",
            "\n",
            "FROM page_view pv;\n",
            "\n",
            "\n",
            "\n",
            "FROM (\n",
            "\n",
            "     FROM pv_users\n",
            "\n",
            "     MAP pv_users.userid, pv_users.date\n",
            "\n",
            "     USING 'map_script'\n",
            "\n",
            "     AS dt, uid\n",
            "\n",
            "     CLUSTER BY dt) map_output\n",
            "\n",
            " \n",
            "\n",
            " INSERT OVERWRITE TABLE pv_users_reduced\n",
            "\n",
            "     REDUCE map_output.dt, map_output.uid\n",
            "\n",
            "     USING 'reduce_script'\n",
            "\n",
            "     AS date, count;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sample map script (weekday_mapper.py )\n",
            "\n",
            "import sys\n",
            "\n",
            "import datetime\n",
            "\n",
            " \n",
            "\n",
            "for line in sys.stdin:\n",
            "\n",
            "  line = line.strip()\n",
            "\n",
            "  userid, unixtime = line.split('\\t')\n",
            "\n",
            "  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()\n",
            "\n",
            "  print ','.join([userid, str(weekday)])\n",
            "\n",
            "\n",
            "\n",
            "Of course, both MAP and REDUCE are \"syntactic sugar\" for the more general select transform. The inner query could also have been written as \n",
            "\n",
            "\n",
            "\n",
            "such:\n",
            "\n",
            "SELECT TRANSFORM(pv_users.userid, pv_users.date) USING 'map_script' AS dt, uid CLUSTER BY dt FROM pv_users;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Schema-less map/reduce: If there is no \"AS\" clause after \"USING map_script\", Hive assumes the output of the script contains 2 parts: key which \n",
            "\n",
            "\n",
            "\n",
            "is before the first tab, and value which is the rest after the first tab. Note that this is different from specifying \"AS key, value\" because \n",
            "\n",
            "\n",
            "\n",
            "in that case value will only contains the portion between the first tab and the second tab if there are multiple tabs.\n",
            "\n",
            "In this way, we allow users to migrate old map/reduce scripts without knowing the schema of the map output. User still needs to know the reduce \n",
            "\n",
            "\n",
            "\n",
            "output schema because that has to match what is in the table that we are inserting to.\n",
            "\n",
            "FROM (\n",
            "\n",
            "    FROM pv_users\n",
            "\n",
            "    MAP pv_users.userid, pv_users.date\n",
            "\n",
            "    USING 'map_script'\n",
            "\n",
            "    CLUSTER BY key) map_output\n",
            "\n",
            " \n",
            "\n",
            "INSERT OVERWRITE TABLE pv_users_reduced\n",
            "\n",
            " \n",
            "\n",
            "    REDUCE map_output.dt, map_output.uid\n",
            "\n",
            "    USING 'reduce_script'\n",
            "\n",
            "    AS date, count;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Distribute By and Sort By: Instead of specifying \"cluster by\", the user can specify \"distribute by\" and \"sort by\", so the partition columns and \n",
            "\n",
            "\n",
            "\n",
            "sort columns can be different. The usual case is that the partition columns are a prefix of sort columns, but that is not required.\n",
            "\n",
            "FROM (\n",
            "\n",
            "    FROM pv_users\n",
            "\n",
            "    MAP pv_users.userid, pv_users.date\n",
            "\n",
            "    USING 'map_script'\n",
            "\n",
            "    AS c1, c2, c3\n",
            "\n",
            "    DISTRIBUTE BY c2\n",
            "\n",
            "    SORT BY c2, c1) map_output\n",
            "\n",
            " \n",
            "\n",
            "INSERT OVERWRITE TABLE pv_users_reduced\n",
            "\n",
            " \n",
            "\n",
            "    REDUCE map_output.c1, map_output.c2, map_output.c3\n",
            "\n",
            "    USING 'reduce_script'\n",
            "\n",
            "    AS date, count;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Co-Groups\n",
            "\n",
            "Amongst the user community using map/reduce, cogroup is a fairly common operation wherein the data from multiple tables are sent to a custom \n",
            "\n",
            "\n",
            "\n",
            "reducer such that the rows are grouped by the values of certain columns on the tables. With the UNION ALL operator and the CLUSTER BY \n",
            "\n",
            "\n",
            "\n",
            "specification, this can be achieved in the Hive query language in the following way. Suppose we wanted to cogroup the rows from the \n",
            "\n",
            "\n",
            "\n",
            "actions_video and action_comments table on the uid column and send them to the 'reduce_script' custom reducer, the following syntax can be used \n",
            "\n",
            "\n",
            "\n",
            "by the user:\n",
            "\n",
            "FROM (\n",
            "\n",
            "     FROM (\n",
            "\n",
            "             FROM action_video av\n",
            "\n",
            "             SELECT av.uid AS uid, av.id AS id, av.date AS date\n",
            "\n",
            " \n",
            "\n",
            "            UNION ALL\n",
            "\n",
            " \n",
            "\n",
            "             FROM action_comment ac\n",
            "\n",
            "             SELECT ac.uid AS uid, ac.id AS id, ac.date AS date\n",
            "\n",
            "     ) union_actions\n",
            "\n",
            "     SELECT union_actions.uid, union_actions.id, union_actions.date\n",
            "\n",
            "     CLUSTER BY union_actions.uid) map\n",
            "\n",
            " \n",
            "\n",
            " INSERT OVERWRITE TABLE actions_reduced\n",
            "\n",
            "     SELECT TRANSFORM(map.uid, map.id, map.date) USING 'reduce_script' AS (uid, id, reduced_val);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "CREATE INDEX x ON TABLE assignment(states)\n",
            "\n",
            "\n",
            "\n",
            "AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "-------------+\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Alter table -alter table tablename rename to newtablename;\n",
            "\n",
            "\n",
            "\n",
            "alter colomn-alter table tablename ADD comumns (doj int, emphike int);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "UPDATE data_point \n",
            "\n",
            "SET dp_name='Audit Related Fees'\n",
            "\n",
            "WHERE dp_id=5;\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "date function-\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Views-create view viewname on tablename select clm,clm2;where clause supported\n",
            "\n",
            "\n",
            "\n",
            "index-create index indexname on table table name (clmname1,clmname2) as 'compact' with differed rebuild;\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Create database PRAFUL;\n",
            "\n",
            "create database PRAFUL comment 'DBname';\n",
            "\n",
            "show databases;\n",
            "\n",
            "use database DBNAME;\n",
            "\n",
            "drop database dbname;\n",
            "\n",
            "To know type of table :\n",
            "\n",
            "describe formatted emp;\n",
            "\n",
            "http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx\n",
            "\n",
            "create external table emp_et(empno int, ename string, job string, sal float, comm float, deptno int) row format delimited fields terminated by \n",
            "\n",
            "',' location '/user/training/emp_mt'\n",
            "\n",
            "create external table emp_etl(empno int, ename string, job string, sal float, comm float, deptno int)\n",
            "\n",
            "row format\n",
            "\n",
            "delimited fields terminated by ','\n",
            "\n",
            "lines terminated by '\\n'\n",
            "\n",
            "stored as TEXTFILE\n",
            "\n",
            "location '/user/training/emp_mt';\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE test_change (a int, b int, c int);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "// First change column a's name to a1.\n",
            "\n",
            "\n",
            "\n",
            "ALTER TABLE test_change CHANGE a a1 INT;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "// Next change column a1's name to a2, its data type to string, and put it after column b.\n",
            "\n",
            "\n",
            "\n",
            "ALTER TABLE test_change CHANGE a1 a2 STRING AFTER b;\n",
            "\n",
            "\n",
            "\n",
            "// The new table's structure is:  b int, a2 string, c int.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "// Then change column c's name to c1, and put it as the first column.\n",
            "\n",
            "\n",
            "\n",
            "ALTER TABLE test_change CHANGE c c1 INT FIRST;\n",
            "\n",
            "\n",
            "\n",
            "// The new table's structure is:  c1 int, b int, a2 string.\n",
            "\n",
            "\n",
            "\n",
            "Importing the structure of another table in hive\n",
            "\n",
            "\n",
            "\n",
            "create table emp3 as select * from empl where 1=2;\n",
            "\n",
            "\n",
            "\n",
            "mysql -u root -p\n",
            "\n",
            "\n",
            "\n",
            "create table emp2 like emp1;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Structure with data\n",
            "\n",
            "\n",
            "\n",
            "create table emp3 as select * from empl;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Truncating data in hive table without truncate command;\n",
            "\n",
            "\n",
            "\n",
            "insert overwrite table empl2 select * from empl where 1=2;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Inserting data in to a hive table\n",
            "\n",
            "\n",
            "\n",
            "insert into table empl2 select * from empl;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Alter table\n",
            "\n",
            "\n",
            "\n",
            "alter table empl2 add columns (bonus float);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "******************* PARTITIONS ***********************\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Static partions\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "load data local inpath '/home/training/praful/student' into table student_new partition(yop='2012');\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "create table st(name string, sno int) partitioned by (yop int) row format delimited fields terminated by ',';\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "load data local inpath '/home/training/PRAFUL/std_2011' into table st partition(yop='2011');\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "load data local inpath '/home/training/PRAFUL/std_2012' into table st partition(yop='2012');\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "\n",
            "Dynamic Partitions\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "set hive.exec.dynamic.partition.mode=nonstrict;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "create table std_temp (name string, id1 int, yop int) row format delimited fields terminated by ',';\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "load data local inpath '/home/training/dvs/std_det' into table std_temp;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "create table std_par(name string, id1 int) partitioned by (yop int) row format delimited fields terminated by ',';\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "show partitions std_par;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "insert overwrite table ip_partition  Partition(country) select * from ip_data; \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "How to add partitions ?\n",
            "\n",
            "\n",
            "\n",
            "Drop partitons ?\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hive -e 'show partitions std1_part;' > partitions\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hive -e 'select * from std1_part;' > sampledata\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hive -f '/home/training/hivefile.txt'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " hive --version\n",
            "\n",
            "\n",
            "\n",
            "select A.ename, B.dname, B.dloc from emp A join dept B ON (A.deptno=B.deptno);\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "select A.* from emp A join dept B ON (A.deptno=B.deptno);\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "select A.ename, A.sal, B.dname, B.dloc from emp A left outer join dept B ON (A.deptno=B.deptno);\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "select A.ename, A.sal, B.dname, B.dloc from emp A right outer join dept B ON (A.deptno=B.deptno);\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&Examples &&&&&&&&&&&&&&&&&&&&&&&&\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE sales (\n",
            "\n",
            "\n",
            "\n",
            "    sales_order_id  BIGINT,\n",
            "\n",
            "\n",
            "\n",
            "    order_amount    FLOAT,\n",
            "\n",
            "\n",
            "\n",
            "    order_date      STRING,\n",
            "\n",
            "\n",
            "\n",
            "    due_date        STRING,\n",
            "\n",
            "\n",
            "\n",
            "    customer_id     BIGINT\n",
            "\n",
            "\n",
            "\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "PARTITIONED BY (country STRING, year INT, month INT, day INT) ;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "INSERT INTO sales\n",
            "\n",
            "\n",
            "\n",
            "PARTITION (country = 'US', year = 2012, month = 12, day = 22)\n",
            "\n",
            "\n",
            "\n",
            "SELECT  sales_order_id,\n",
            "\n",
            "\n",
            "\n",
            "        order_amount,\n",
            "\n",
            "\n",
            "\n",
            "        due_date,\n",
            "\n",
            "\n",
            "\n",
            "        customer_id,\n",
            "\n",
            "\n",
            "\n",
            "        cntry,\n",
            "\n",
            "\n",
            "\n",
            "        yr,\n",
            "\n",
            "\n",
            "\n",
            "        mo,\n",
            "\n",
            "\n",
            "\n",
            "        d\n",
            "\n",
            "\n",
            "\n",
            "FROM    source_view\n",
            "\n",
            "\n",
            "\n",
            "WHERE   cntry = 'US'\n",
            "\n",
            "\n",
            "\n",
            "        AND yr = 2012\n",
            "\n",
            "\n",
            "\n",
            "        AND mo = 12\n",
            "\n",
            "\n",
            "\n",
            "        AND d = 22 ;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "INSERT INTO sales\n",
            "\n",
            "\n",
            "\n",
            "PARTITION (country, year, month, day)\n",
            "\n",
            "\n",
            "\n",
            "SELECT  sales_order_id,\n",
            "\n",
            "\n",
            "\n",
            "        order_amount,\n",
            "\n",
            "\n",
            "\n",
            "        due_date,\n",
            "\n",
            "\n",
            "\n",
            "        customer_id,\n",
            "\n",
            "\n",
            "\n",
            "        cntry,\n",
            "\n",
            "\n",
            "\n",
            "        yr,\n",
            "\n",
            "\n",
            "\n",
            "        mo,\n",
            "\n",
            "\n",
            "\n",
            "        d\n",
            "\n",
            "\n",
            "\n",
            "FROM    source_view\n",
            "\n",
            "\n",
            "\n",
            "WHERE   cntry = 'US' ;\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************\n",
            "\n",
            "\n",
            "\n",
            "ename, substr(ename,1,4), substr(ename,5), substr(ename,-5), instr(ename,'AL'), instr(ename, 'A')\n",
            "\n",
            "\n",
            "\n",
            "find_in_set('SMITH',ename)\n",
            "\n",
            "\n",
            "\n",
            "select mail_id, substr(mail_id, 1, instr(mail_id,'.')-1) first_name, substr(mail_id, instr(mail_id,'.')+1, instr(mail_id,'@')-instr\n",
            "\n",
            "\n",
            "\n",
            "(mail_id,'.')-1) last_name from email;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Number Functions :\n",
            "\n",
            "\n",
            "\n",
            "1. Ceil, Floor, round, round(sal,2), sqrt, abs, power(10,2),\n",
            "\n",
            "\n",
            "\n",
            "2. Aggregate Functions\n",
            "\n",
            "\n",
            "\n",
            "                sum, min, max, avg, count\n",
            "\n",
            "\n",
            "\n",
            "3. Nested Functions round(max(sal))\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Operators:\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "1. Relational Operators:\n",
            "\n",
            "\n",
            "\n",
            "=, !=, <, >, <=, >=, and, or, like, not like, between, is null, is not null\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "2. Arithmatic Operators (+,-,*,/,%)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "DDL in HIVE:\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Drop\n",
            "\n",
            "\n",
            "\n",
            "Rename\n",
            "\n",
            "\n",
            "\n",
            "Create\n",
            "\n",
            "\n",
            "\n",
            "Alter\n",
            "\n",
            "\n",
            "\n",
            "Truncate\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Drop table emp;\n",
            "\n",
            "\n",
            "\n",
            "Rename Table : Alter table emp rename to emp1;\n",
            "\n",
            "\n",
            "\n",
            "Add Column : alter table emp00 add columns(bonus int);\n",
            "\n",
            "\n",
            "\n",
            "Rename Column : alter table emp1 change bonus bonus1 int;\n",
            "\n",
            "\n",
            "\n",
            "Change data type : alter table emp00 change bonus1 bonus1 float first;\n",
            "\n",
            "\n",
            "\n",
            "first, after\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "alter table t1 replace columns (c string, d int);\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "1. Table creation with row format, lines, type of file\n",
            "\n",
            "\n",
            "\n",
            "2. Try compressed file with sequence file.\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "3. hive -e 'show tables';\n",
            "\n",
            "\n",
            "\n",
            "4. hive -e 'select deptno, sum(sal), min(sal), max(sal), count(sal), avg(sal) from emp_mt group by deptno'>empdet;\n",
            "\n",
            "\n",
            "\n",
            "5. hive -f hivescript\n",
            "\n",
            "\n",
            "\n",
            "6. hive -f hivescript>hive_out\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "grant select to user training;\n",
            "\n",
            "\n",
            "\n",
            "show grant user training;\n",
            "\n",
            "\n",
            "\n",
            "grant all on database dvsdb to user training;\n",
            "\n",
            "\n",
            "\n",
            "show grant user training on database dvsdb;\n",
            "\n",
            "\n",
            "\n",
            "revoke select on table emp_mt from user training;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hive -e 'select * from dvsdb.emp_mt11 where deptno=10'>dept100;\n",
            "\n",
            "\n",
            "\n",
            "hive -e 'select * from dvsdb.emp_mt11 where deptno=20'>dept200;\n",
            "\n",
            "\n",
            "\n",
            "hive -e 'select * from dvsdb.emp_mt11 where comm is null and sal<1500'>abc\n",
            "\n",
            "\n",
            "\n",
            "hive -e 'select * from dvsdb.emp_mt11 where comm is null'>comm_null\n",
            "\n",
            "\n",
            "\n",
            "hive -e 'select * from dvsdb.emp_mt11 where sal between 1000 and 2000'>salrange;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "External Table without location :\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "create external table emp_et (empno int, ename string, job string, sal double, comm double, deptno int) row format delimited fields terminated \n",
            "\n",
            "\n",
            "\n",
            "by ',';\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "CREATE TABLE assignment(i int, j int);\n",
            "\n",
            "\n",
            "\n",
            "CREATE INDEX x ON TABLE assignment(states)\n",
            "\n",
            "\n",
            "\n",
            "AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***********Creating view on entire table :\n",
            "\n",
            "\n",
            "\n",
            "create view emp_view COMMENT 'View on emp table' as select * from emp;\n",
            "\n",
            "\n",
            "\n",
            "*********Creating view on selected columns of the tabe:\n",
            "\n",
            "create view dept_view as select deptno, dname from dept;\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "***********Creating view on joining 2 tables :\n",
            "\n",
            "\n",
            "\n",
            "create view emp_dept_view as select A.empno, B.deptno, B.dname from emp A join dept B where A.deptno=B.deptno;\n",
            "\n",
            "\n",
            "\n",
            "***********Creating view from a existing view :\n",
            "\n",
            "\n",
            "\n",
            "***********Drop view\n",
            "\n",
            " \n",
            "\n",
            "we acan use like operator for here as well\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hive> SHOW DATABASES LIKE 'h.*';\n",
            "\n",
            "\n",
            "\n",
            "human_resources\n",
            "\n",
            "\n",
            "\n",
            "create database in user defined locations\n",
            "\n",
            "\n",
            "\n",
            "hive> CREATE DATABASE financials\n",
            "\n",
            "\n",
            "\n",
            "> LOCATION '/my/preferred/directory';\n",
            "\n",
            "\n",
            "\n",
            "describe database databasename; //  to know more about database\n",
            "\n",
            "\n",
            "\n",
            "to comment database and describe all about the database more\n",
            "\n",
            "\n",
            "\n",
            "hdfs://master-server/user/hive/warehouse/financials.db\n",
            "\n",
            "\n",
            "\n",
            "to know more about the database with creator details;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hive> CREATE DATABASE financials\n",
            "\n",
            "\n",
            "\n",
            "> WITH DBPROPERTIES ('creator' = 'Mark Moneybags', 'date' = '2012-01-02');\n",
            "\n",
            "\n",
            "\n",
            "hive> DESCRIBE DATABASE financials;\n",
            "\n",
            "\n",
            "\n",
            "financials hdfs://master-server/user/hive/warehouse/financials.db\n",
            "\n",
            "\n",
            "\n",
            "hive> DESCRIBE DATABASE EXTENDED financials;\n",
            "\n",
            "\n",
            "\n",
            "financials hdfs://master-server/user/hive/warehouse/financials.db\n",
            "\n",
            "\n",
            "\n",
            "{date=2012-01-02, creator=Mark Moneybags);\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alter databases;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hive> ALTER DATABASE financials SET DBPROPERTIES ('edited-by' = 'Joe Dba');\n",
            "\n",
            "\n",
            "\n",
            "complex ---\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hive>\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "create table if not exists employee (name string comment 'Employee name',salary float comment  'Employee salary',subordinators array <string> \n",
            "\n",
            "\n",
            "\n",
            "comment\n",
            "\n",
            "\n",
            "\n",
            "'Name of Subordinates',deductions map <string,float> comment 'Employee Deductions',\n",
            "\n",
            "\n",
            "\n",
            "address struct <street:string,city:string,states:string,zip:int> comment 'home adress') comment 'description of the table'\n",
            "\n",
            "\n",
            "\n",
            "tblproperties ('created' = 'PRAFUL', 'date'='28-1-2015','object' = 'to use all complext datatype');\n",
            "\n",
            "\n",
            "\n",
            "to see table properties;\n",
            "\n",
            "\n",
            "\n",
            "> show tblproperties tablename;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "HIVE --- END\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "***********************\n",
            "\n",
            "\n",
            "\n",
            "MAPREDUCE flow Theory\n",
            "\n",
            "\n",
            "\n",
            "***********************\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "hi how are you\n",
            "\n",
            "\n",
            "\n",
            "how is your job\n",
            "\n",
            "\n",
            "\n",
            "how is your temp---7-lines\n",
            "\n",
            "\n",
            "\n",
            "*****************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "file size 200 mb and\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "expected output----hi 1,\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "inputsplit = no of mapper\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Mapper only knows (key,value) pairs\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "lines = record\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "recordreader interface in between inputsplit and mapper\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "************\n",
            "\n",
            "recordreader will read record like as byteoffset entireline\n",
            "\n",
            "\n",
            "\n",
            "for every key,value pair out mapper will run as per lines given\n",
            "\n",
            "\n",
            "\n",
            "parallel processing*****\n",
            "\n",
            "\n",
            "\n",
            "******\n",
            "\n",
            "\n",
            "\n",
            "Map will run on collection frame work;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "*******\n",
            "\n",
            "\n",
            "\n",
            "Java primitivetypes/data types;\n",
            "\n",
            "\n",
            "\n",
            "*******\n",
            "\n",
            "\n",
            "\n",
            "entire collection types work on objective type not primitive types like map function in java\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "objective type mean it should have some class;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "it means above mentioned are able to work with collection framework\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "we need to change the primitive type into obejective type via wrapper class\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "int to Intwriteable  = new IntWriteable(int)\n",
            "\n",
            "\n",
            "\n",
            "IntWriteable to int  = get() //method\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "String to Text = new Text(string)\n",
            "\n",
            "\n",
            "\n",
            "Text to String = ToString()\n",
            "\n",
            "\n",
            "\n",
            "int = 4bytes\n",
            "\n",
            "\n",
            "\n",
            "long = 8bytes\n",
            "\n",
            "\n",
            "\n",
            "for key valueInput format\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "entire line will be value\n",
            "\n",
            "\n",
            "\n",
            "from delimiter will key\n",
            "\n",
            "\n",
            "\n",
            "100         praful\n",
            "\n",
            "\n",
            "\n",
            "100-key\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "take the mapper out put\n",
            "\n",
            "\n",
            "\n",
            "key should not be duplicate,\n",
            "\n",
            "\n",
            "\n",
            "output shd not be duplicate keys\n",
            "\n",
            "\n",
            "\n",
            "value can be\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "for comeout duplicate issue; shuffling, sorting\n",
            "\n",
            "\n",
            "\n",
            "intermidiate data = mapper output\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "shuffling phase is trying to combine all your assiciated values for single identy call key\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "(how[1,1,1,1,1,1])\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "all boxclass are implemented with writeablecomparable interfaces  for sorting as well done because comparable is there\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "servelet passing some request\n",
            "\n",
            "\n",
            "\n",
            "do get method\n",
            "\n",
            "\n",
            "\n",
            "do post method\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "as per the no of request service request\n",
            "\n",
            "\n",
            "\n",
            "***************\n",
            "\n",
            "\n",
            "\n",
            "REDUCER\n",
            "\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "2types of reducer\n",
            "\n",
            "\n",
            "\n",
            "******\n",
            "\n",
            "\n",
            "\n",
            "1-identy reducer-sort is there\n",
            "\n",
            "\n",
            "\n",
            "2-own reducer code- shuffle and sorting\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "ITERATE WILL TAKE CARE OF SORTING AND SHUFFLING\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "THEN REDUCER WILL FINALIZE THE OUT AND GIVE TO RECORD WRITER  KEY VALUE TO O/P FILE WITH -part00000\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "then o/p directory will get 3 file\n",
            "\n",
            "\n",
            "\n",
            "-SUCCESS -okb file with message\n",
            "\n",
            "\n",
            "\n",
            "-log-directory-history of your jobs and xml files details\n",
            "\n",
            "\n",
            "\n",
            "-part00000--output file in a sorting order\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "****************************\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "********************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FLUME\n",
            "\n",
            "\n",
            "\n",
            "**********************\n",
            "\n",
            "\n",
            "\n",
            "Oozie\n",
            "\n",
            "\n",
            "\n",
            "**********************\n",
            "\n",
            "\n",
            "\n",
            "Two nodes are thire\n",
            "\n",
            "\n",
            "\n",
            "Control node ---Where the job will start\n",
            "\n",
            "\n",
            "\n",
            "Action node--How to execute one job at a time\n",
            "\n",
            "\n",
            "\n",
            "there is a configurable file\n",
            "\n",
            "\n",
            "\n",
            "workflow.xml\n",
            "\n",
            "\n",
            "\n",
            "/etc/init.d/Oozie start\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "********************\n",
            "\n",
            "\n",
            "\n",
            "HBASE\n",
            "\n",
            "\n",
            "\n",
            "********************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "componet,\n",
            "\n",
            "Hmaster,\n",
            "\n",
            "HRegion Server\n",
            "\n",
            "Hbase Client\n",
            "\n",
            "based on the row key it will distribute data accorss the cluster\n",
            "\n",
            "Client will co ordinate with zookeeper \n",
            "\n",
            "then Hmaster will will keep communication in between HRS\n",
            "\n",
            "\n",
            "\n",
            "Bloomfiltter\n",
            "\n",
            "Use HColumnDescriptor.setBloomFilterType(NONE | ROW | ROWCOL) to enable blooms per Column Family\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Row /Column\n",
            "\n",
            "\n",
            "\n",
            "RDBMS vs NOSQL-\n",
            "\n",
            "\n",
            "\n",
            "schema oriented nosql no schema concept, vertical scalable, \n",
            "\n",
            "horizontal--so we can many missions then horizontal is requried\n",
            "\n",
            "\n",
            "\n",
            "CAP -Therom( Consistency. Availability. Partition tolerance)\n",
            "\n",
            "History of hbase designed-2004-2007, based on google bigtable\n",
            "\n",
            "\n",
            "\n",
            "Why NOSQL--space and limitations are there\n",
            "\n",
            "\n",
            "\n",
            "types of NOSQL DB's-\n",
            "\n",
            "\n",
            "\n",
            "key value store -Dynamo db\n",
            "\n",
            "\n",
            "\n",
            "Column oriented              -HBASE, CASSANDRA\n",
            "\n",
            "\n",
            "\n",
            "GRAPH Oriented              -NEO4J, Girafi \n",
            "\n",
            "\n",
            "\n",
            "Documented Orint- Mongo DB,rest all are document oriented databases\n",
            "\n",
            "\n",
            "\n",
            "in hbase read high\n",
            "\n",
            "\n",
            "\n",
            "in casandra write high\n",
            "\n",
            "\n",
            "\n",
            "google is used chubby,bigtable and cache database for its search engine\n",
            "\n",
            "\n",
            "\n",
            "http://nosql-database.org/\n",
            "\n",
            "\n",
            "\n",
            "****************\n",
            "\n",
            "\n",
            "\n",
            "Hbase USE's\n",
            "\n",
            "\n",
            "\n",
            "architechture\n",
            "\n",
            "\n",
            "\n",
            "and\n",
            "\n",
            "\n",
            "\n",
            "work flow\n",
            "\n",
            "\n",
            "\n",
            "commands as\n",
            "\n",
            "\n",
            "\n",
            "to start hbase main with server --start-hbase.sh\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "then hbase shell\n",
            "\n",
            "\n",
            "\n",
            "list-- to see tables\n",
            "\n",
            "\n",
            "\n",
            "to create a table\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "create 'batch5' , 'cf'\n",
            "\n",
            "put 'batch5','r1','cf:c1','v1'\n",
            "\n",
            "put 'batch5','r2','cf:c2','v2'\n",
            "\n",
            "put 'batch5','r3','cf:c3','v3'\n",
            "\n",
            "\n",
            "\n",
            "get table name 'rowname'\n",
            "\n",
            "\n",
            "\n",
            "to update\n",
            "\n",
            "\n",
            "\n",
            "put 'batch5','r3','cf:c3','updatesv3' \n",
            "\n",
            "\n",
            "\n",
            "delete 'batch5','r2','cf:c2' \n",
            "\n",
            "\n",
            "\n",
            "writing data into hase table inserting data into table\n",
            "\n",
            "\n",
            "\n",
            "put (tablename)'praful', then column positions 1st,2nd, 'fourth' it may be any, 'cf:any' , message any if string then in qote else digital \n",
            "\n",
            "\n",
            "\n",
            "value\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(put 'praful' ,'290', 'cf:foo',456)\n",
            "\n",
            "\n",
            "\n",
            "**************\n",
            "\n",
            "\n",
            "\n",
            "reading data\n",
            "\n",
            "\n",
            "\n",
            "************\n",
            "\n",
            "\n",
            "\n",
            "For Entire Table fetch\n",
            "\n",
            "\n",
            "\n",
            "scan tablename in qote\n",
            "\n",
            "\n",
            "\n",
            "scan 'praful'\n",
            "\n",
            "\n",
            "\n",
            "get (table name), then where conditions\n",
            "\n",
            "\n",
            "\n",
            "get 'praful','first'\n",
            "\n",
            "\n",
            "\n",
            "********\n",
            "\n",
            "\n",
            "\n",
            "describe\n",
            "\n",
            "\n",
            "\n",
            "*********\n",
            "\n",
            "rowkey like primary key  \n",
            "\n",
            "Column family\n",
            "\n",
            "column grouped under one CF\n",
            "\n",
            "TIMESTAMP - value is maintain by Hbase\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "create 'batch5', \n",
            "\n",
            "{NAME=>'info',VERSION=>1}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6. MR using Python\n",
            "\n",
            "\n",
            "\n",
            "trim function\n",
            "\n",
            "\n",
            "\n",
            "our class must extend UDF abstract class\n",
            "\n",
            "\n",
            "\n",
            "our class must have atleast one evaluate method-different class with one parameter\n",
            "\n",
            "\n",
            "\n",
            "compile the above class/create .java file\n",
            "\n",
            "\n",
            "\n",
            "create jar file for keeping .class file\n",
            "\n",
            "\n",
            "\n",
            "add jar file to hive class path\n",
            "\n",
            "\n",
            "\n",
            "create temporary function\n",
            "\n",
            "\n",
            "\n",
            "go to java\n",
            "\n",
            "\n",
            "\n",
            "create a project\n",
            "\n",
            "\n",
            "\n",
            "add jars\n",
            "\n",
            "\n",
            "\n",
            "hadoop core jars\n",
            "\n",
            "\n",
            "\n",
            "hive jars\n",
            "\n",
            "\n",
            "\n",
            "create class\n",
            "\n",
            "\n",
            "\n",
            "give ur package name\n",
            "\n",
            "\n",
            "\n",
            "then class name- testudf\n",
            "\n",
            "\n",
            "\n",
            "extends UDF\n",
            "\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "\n",
            "package com.hadoop.hive;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "import org.apache.commons.lang.StringUtils;\n",
            "\n",
            "\n",
            "\n",
            "import org.apache.hadoop.hive.ql.exec.UDF;\n",
            "\n",
            "\n",
            "\n",
            "import org.apache.hadoop.io.Text;\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "public class testudf extends UDF {\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "                Text t = new Text();\n",
            "\n",
            "\n",
            "\n",
            "                public Text evaluate(Text str){\n",
            "\n",
            "\n",
            "\n",
            "                                if (str==null){\n",
            "\n",
            "\n",
            "\n",
            "                                                return str;\n",
            "\n",
            "\n",
            "\n",
            "                                }\n",
            "\n",
            "\n",
            "\n",
            "                                t.set(StringUtils.strip(str.toString()));\n",
            "\n",
            "\n",
            "\n",
            "                                                return t;\n",
            "\n",
            "\n",
            "\n",
            "                                               \n",
            "\n",
            "\n",
            "\n",
            "                }\n",
            "\n",
            "\n",
            "\n",
            "               \n",
            "\n",
            "\n",
            "\n",
            "                public Text evaluate(Text str,String splchar){\n",
            "\n",
            "\n",
            "\n",
            "                                if (str==null){\n",
            "\n",
            "\n",
            "\n",
            "                                                return str;\n",
            "\n",
            "\n",
            "\n",
            "                                }\n",
            "\n",
            "\n",
            "\n",
            "                                t.set(StringUtils.strip(str.toString(),splchar));\n",
            "\n",
            "\n",
            "\n",
            "                                                return t;\n",
            "\n",
            "\n",
            "\n",
            "                                               \n",
            "\n",
            "\n",
            "\n",
            "                }\n",
            "\n",
            "\n",
            "\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "-----\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "add jar '/home/training/hd_workspace/hive.example.jar';\n",
            "\n",
            "\n",
            "\n",
            "create temporary fn\n",
            "\n",
            "\n",
            "\n",
            "create temporary function praful as 'com.hadoop.hive.Testudf';\n",
            "\n",
            "\n",
            "\n",
            "use it\n",
            "\n",
            "\n",
            "\n",
            "select praful (functionname) (fname) from table;\n",
            "\n",
            "\n",
            "\n",
            "********\n",
            "\n",
            "\n",
            "\n",
            "issue in 1x\n",
            "\n",
            "\n",
            "\n",
            "namenode-\n",
            "\n",
            "\n",
            "\n",
            "wndws\n",
            "\n",
            "\n",
            "\n",
            "lnx\n",
            "\n",
            "\n",
            "\n",
            "multiplenamenode\n",
            "\n",
            "\n",
            "\n",
            "limit of add nodes-2000\n",
            "\n",
            "\n",
            "\n",
            "namenodefdrtn--metadata distrbtd-mu-ltiplenamenode\n",
            "\n",
            "\n",
            "\n",
            "namenodehigh availibility-secndry is optinal-2nn\n",
            "\n",
            "\n",
            "\n",
            "namenodejurnalism--maintain commucation of all,who is active\n",
            "\n",
            "\n",
            "\n",
            "zookeeper-monitor-general nodes make communication\n",
            "\n",
            "\n",
            "\n",
            "2sec,\n",
            "\n",
            "\n",
            "\n",
            "mapreduce--\n",
            "\n",
            "\n",
            "\n",
            "db-1jt-scheduling jobs-\n",
            "\n",
            "\n",
            "\n",
            "2-resouce alocation\n",
            "\n",
            "\n",
            "\n",
            "tt -on all dn-\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "package sqooptest;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "import net.neoremind.sshxcute.core.SSHExec;\n",
            "\n",
            "import net.neoremind.sshxcute.core.ConnBean;\n",
            "\n",
            "import net.neoremind.sshxcute.task.CustomTask;\n",
            "\n",
            "import net.neoremind.sshxcute.task.impl.ExecCommand;\n",
            "\n",
            "\n",
            "\n",
            "public class TestSSH {\n",
            "\n",
            "\n",
            "\n",
            "public static void main(String args[]) throws Exception{\n",
            "\n",
            "\n",
            "\n",
            "    // Initialize a ConnBean object, parameter list is ip, username, password\n",
            "\n",
            "\n",
            "\n",
            "    ConnBean cb = new ConnBean(\"192.168.56.102\", \"root\",\"hadoop\");\n",
            "\n",
            "\n",
            "\n",
            "    // Put the ConnBean instance as parameter for SSHExec static method getInstance(ConnBean) to retrieve a singleton SSHExec instance\n",
            "\n",
            "    SSHExec ssh = SSHExec.getInstance(cb);          \n",
            "\n",
            "    // Connect to server\n",
            "\n",
            "    ssh.connect();\n",
            "\n",
            "    CustomTask sampleTask1 = new ExecCommand(\"echo $SSH_CLIENT\"); // Print Your Client IP By which you connected to ssh server on Horton \n",
            "\n",
            "\n",
            "\n",
            "Sandbox\n",
            "\n",
            "    System.out.println(ssh.exec(sampleTask1));\n",
            "\n",
            "    CustomTask sampleTask2 = new ExecCommand(\"sqoop import --connect jdbc:mysql://192.168.56.101:3316/mysql_db_name --username=mysql_user --\n",
            "\n",
            "\n",
            "\n",
            "password=mysql_pwd --table mysql_table_name --hive-import -m 1 -- --schema default\");\n",
            "\n",
            "    ssh.exec(sampleTask2);\n",
            "\n",
            "    ssh.disconnect();   \n",
            "\n",
            "}\n",
            "\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            " CustomTask sampleTask2 = new ExecCommand(\"<installation_path>/bin/sqoop import --connect jdbc:oracle:thin:@172.17.13.47:1521/xe --username \n",
            "\n",
            "\n",
            "\n",
            "ouser --password oracleuser --table BRANCH --target-dir /user/branch -m 1\")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " java -cp praful.jar:$SQOOP_HOME/*:$SQOOP_HOME/lib/*:$HADOOP_PREFIX/*:$HADOOP_PREFIX/lib/* sqooptest.SqoopTest\n",
            "\n",
            "\n",
            "\n",
            "----------------------\n",
            "\n",
            "\n",
            "\n",
            "******************************\n",
            "\n",
            "\n",
            "\n",
            "Knime\n",
            "\n",
            "\n",
            "\n",
            "******************************\n",
            "\n",
            "\n",
            "\n",
            "import oracle.jdbc.OracleDriver;\n",
            "\n",
            "import java.sql.Connection;\n",
            "\n",
            "import java.sql.Statement;\n",
            "\n",
            "import java.sql.ResultSet;\n",
            "\n",
            "import java.sql.DriverManager;\n",
            "\n",
            "import java.sql.Timestamp;\n",
            "\n",
            "import java.util.Calendar;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "try {\n",
            "\n",
            "\tClass.forName(\"oracle.jdbc.OracleDriver\");\n",
            "\n",
            "\tConnection conn = DriverManager.getConnection(\"jdbc:oracle:thin:@172.19.36.144:1521:recon\",\"knime_testing\",\"knime\");\n",
            "\n",
            "\tStatement stmt = conn.createStatement();\n",
            "\n",
            "\tResultSet rs = stmt.executeQuery(\"select recon_oid_seq.nextval from dual\");\n",
            "\n",
            "\trs.next();\n",
            "\n",
            "\tout_RECON_OID = rs.getInt(1);\n",
            "\n",
            "\tstmt.close();\n",
            "\n",
            "\tconn.close();\n",
            "\n",
            "\tout_CREATED_DATE = new Timestamp(Calendar.getInstance().getTimeInMillis());\n",
            "\n",
            "\tout_RECORD_STATUS = \"ACTIVE\";\n",
            "\n",
            "\tout_CREATED_BY = \"SYSTEM\";\n",
            "\n",
            "\tout_RECORD_VERSION = 1;\n",
            "\n",
            "\tout_IPADDRESS = \"0.0.0.0\";\n",
            "\n",
            "\tout_RECONCILIATION_TYPE = \"AUTO\";\n",
            "\n",
            "\tout_BASE_CURRENCY = \"INR\";\n",
            "\n",
            "} catch(Exception e) {\n",
            "\n",
            "\te.printStackTrace();\n",
            "\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "java -cp praful.jar:$SQOOP_HOME/*:$SQOOP_HOME/lib/*:$HADOOP_PREFIX/*:$HADOOP_PREFIX/lib/* sqooptest.SqoopTest\n",
            "\n",
            "\n",
            "\n",
            "*******************\n",
            "\n",
            "oozie\n",
            "\n",
            "*****\n",
            "\n",
            "1 dwnload the sw from apache\n",
            "\n",
            "\n",
            "\n",
            "exctract it on hadoop env\n",
            "\n",
            "cd oozie-3.3.2/bin\n",
            "\n",
            "************UAT**************\n",
            "\n",
            "\n",
            "\n",
            "172.19.36.224\n",
            "\n",
            "172.19.36.225\n",
            "\n",
            "awk -F':' '{ print $1}' /etc/passwd\n",
            "\n",
            "\n",
            "\n",
            "scp ambari.repo hduser@172.19.36.225:/home/hduser\n",
            "\n",
            "HADOOP http://recon224/HDP/centos6/2.x/updates/2.3.0.0/\n",
            "\n",
            "  \n",
            "\n",
            " cat /sys/kernel/mm/redhat_transparent_hugepage/defrag always madvise [never]\n",
            "\n",
            " jdbc:oracle:thin:@recon225.idfc.com:1521/hive\n",
            "\n",
            " jdbc:oracle:thin:@172.19.36.144:1521:recon\n",
            "\n",
            "\n",
            "\n",
            "8339946956-kanha\n",
            "\n",
            "\n",
            "\n",
            "local_hdfs=hdfs\n",
            "\n",
            "\n",
            "\n",
            "local.host=192.168.241.18\n",
            "\n",
            "local.userid=tcsftp100\n",
            "\n",
            "local.password=tcsftp100\n",
            "\n",
            "local.filepath=REPORTS/00001\n",
            "\n",
            "\n",
            "\n",
            "local.host=192.168.1.11\n",
            "\n",
            "local.userid=hduser\n",
            "\n",
            "local.password=h@D00p\n",
            "\n",
            "local.filepath=temp\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hdfs.host=172.19.36.143\n",
            "\n",
            "hdfs.userid=oozie\n",
            "\n",
            "hdfs.password=oozie\n",
            "\n",
            "hdfs.filepath=/user/oozie/jobs/\n",
            "\n",
            "hdfs.portno=8020\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "###   Hadoop 1.0 + sarath machine settings\n",
            "\n",
            "\n",
            "\n",
            "#local.host=192.168.1.11\n",
            "\n",
            "#local.userid=hduser\n",
            "\n",
            "#local.password=h@D00p\n",
            "\n",
            "#local.filepath=temp\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#hdfs.host=192.168.1.8\n",
            "\n",
            "#hdfs.userid=hduser\n",
            "\n",
            "#hdfs.password=hduser\n",
            "\n",
            "#hdfs.filepath=/user/cloumon/jobs\n",
            "\n",
            "#hdfs.portno=54310\n",
            "\n",
            "\n",
            "\n",
            "curl -i -X PUT \"http://172.19.36.143:50070/webhdfs/v1/user/hduser/AFTRP/recon_jobs/1111297312/CLG_CASH_APAC_18185/conf?\n",
            "\n",
            "\n",
            "\n",
            "user.name=supergroup&op=MKDIRS\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "/user/hduser/AFTRP/recon_jobs/1111297312/CLG_CASH_APAC_18185/conf\n",
            "\n",
            "curl -i \"http://172.19.36.143:50070/webhdfs/v1/user?op=GETCONTENTSUMMARY\"\n",
            "\n",
            "\n",
            "\n",
            "curl -i -X PUT \"http://<HOST>:<PORT>/webhdfs/v1/user?op=SETREPLICATION &replication=5\"\n",
            "\n",
            "\n",
            "\n",
            "curl http://172.19.36.143:14000/webhdfs/v1/user/foo/README.txt returns the contents of the HDFS /user/foo/README.txt file\n",
            "\n",
            "\n",
            "\n",
            "hdfs dfs -getfacl [-R] /user/praful\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  <property>\n",
            "\n",
            "      <name>dfs.client.retry.policy.enabled</name>\n",
            "\n",
            "      <value>false</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.cluster.administrators</name>\n",
            "\n",
            "      <value> hdfs</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.address</name>\n",
            "\n",
            "      <value>0.0.0.0:50010</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.balance.bandwidthPerSec</name>\n",
            "\n",
            "      <value>6250000</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.data.dir</name>\n",
            "\n",
            "      <value>/hadoop/hdfs/data</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.data.dir.perm</name>\n",
            "\n",
            "      <value>750</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.du.reserved</name>\n",
            "\n",
            "      <value>1073741824</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.failed.volumes.tolerated</name>\n",
            "\n",
            "      <value>0</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.http.address</name>\n",
            "\n",
            "      <value>0.0.0.0:50075</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.https.address</name>\n",
            "\n",
            "      <value>0.0.0.0:50475</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.ipc.address</name>\n",
            "\n",
            "      <value>0.0.0.0:8010</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.datanode.max.transfer.threads</name>\n",
            "\n",
            "      <value>4096</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "  <property>\n",
            "\n",
            "      <name>dfs.domain.socket.path</name>\n",
            "\n",
            "      <value>/var/lib/hadoop-hdfs/dn_socket</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.encrypt.data.transfer.cipher.suites</name>\n",
            "\n",
            "      <value>AES/CTR/NoPadding</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.encryption.key.provider.uri</name>\n",
            "\n",
            "      <value></value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.heartbeat.interval</name>\n",
            "\n",
            "      <value>3</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.hosts.exclude</name>\n",
            "\n",
            "      <value>/etc/hadoop/conf/dfs.exclude</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.http.policy</name>\n",
            "\n",
            "      <value>HTTP_ONLY</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.https.port</name>\n",
            "\n",
            "      <value>50470</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.journalnode.edits.dir</name>\n",
            "\n",
            "      <value>/hadoop/hdfs/journalnode</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.journalnode.http-address</name>\n",
            "\n",
            "      <value>0.0.0.0:8480</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.journalnode.https-address</name>\n",
            "\n",
            "      <value>0.0.0.0:8481</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.accesstime.precision</name>\n",
            "\n",
            "      <value>0</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.audit.log.async</name>\n",
            "\n",
            "      <value>true</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.avoid.read.stale.datanode</name>\n",
            "\n",
            "      <value>true</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.avoid.write.stale.datanode</name>\n",
            "\n",
            "      <value>true</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.checkpoint.dir</name>\n",
            "\n",
            "      <value>/hadoop/hdfs/namesecondary</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.checkpoint.edits.dir</name>\n",
            "\n",
            "      <value>${dfs.namenode.checkpoint.dir}</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.checkpoint.period</name>\n",
            "\n",
            "      <value>21600</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.checkpoint.txns</name>\n",
            "\n",
            "      <value>1000000</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.fslock.fair</name>\n",
            "\n",
            "      <value>false</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.handler.count</name>\n",
            "\n",
            "      <value>100</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.http-address</name>\n",
            "\n",
            "      <value>recon143.idfc.com:50070</value>\n",
            "\n",
            "      <final>true</final>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.https-address</name>\n",
            "\n",
            "      <value>recon143.idfc.com:50470</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.name.dir</name>\n",
            "\n",
            "      <value>/hadoop/hdfs/namenode</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.name.dir.restore</name>\n",
            "\n",
            "      <value>true</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.rpc-address</name>\n",
            "\n",
            "      <value>recon143.idfc.com:8020</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.safemode.threshold-pct</name>\n",
            "\n",
            "      <value>1</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.secondary.http-address</name>\n",
            "\n",
            "      <value>recon142.idfc.com:50090</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.stale.datanode.interval</name>\n",
            "\n",
            "      <value>30000</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.startup.delay.block.deletion.sec</name>\n",
            "\n",
            "      <value>3600</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.namenode.write.stale.datanode.ratio</name>\n",
            "\n",
            "      <value>1.0f</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.permissions.enabled</name>\n",
            "\n",
            "      <value>false</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.permissions.superusergroup</name>\n",
            "\n",
            "      <value>hdfs</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.replication</name>\n",
            "\n",
            "      <value>3</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.replication.max</name>\n",
            "\n",
            "      <value>50</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.support.append</name>\n",
            "\n",
            "      <value>true</value>\n",
            "\n",
            "      <final>true</final>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>dfs.webhdfs.enabled</name>\n",
            "\n",
            "      <value>true</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>fs.permissions.umask-mode</name>\n",
            "\n",
            "      <value>022</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>nfs.exports.allowed.hosts</name>\n",
            "\n",
            "      <value>* rw</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "    <property>\n",
            "\n",
            "      <name>nfs.file.dump.dir</name>\n",
            "\n",
            "      <value>/tmp/.hdfs-nfs</value>\n",
            "\n",
            "    </property>\n",
            "\n",
            "\n",
            "\n",
            "  </configuration>\n",
            "\n",
            "  \n",
            "\n",
            "  sudo vi /usr/hdp/2.3.0.0-2557/hadoop/conf/hdfs-site.xml\n",
            "\n",
            "   \n",
            "\n",
            "   <property>\n",
            "\n",
            "           <name>dfs.webhdfs.enabled</name>\n",
            "\n",
            "           <value>true</value>\n",
            "\n",
            "        </property>\n",
            "\n",
            "\n",
            "\n",
            "\t\t\n",
            "\n",
            "\t\t\t\t\t\n",
            "\n",
            "\t\t*************CURL*************\n",
            "\n",
            "\t\t\n",
            "\n",
            "\t\t\n",
            "\n",
            "\t\tWebHDFS examples\n",
            "\n",
            "As the simplest approach, we can use curl to invoke WebHDFS REST API\n",
            "\n",
            "\n",
            "\n",
            "1./ Check directory status\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i \"http://localhost:50070/webhdfs/v1/tmp?user.name=istvan&op=GETFILESTATUS\"\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "\n",
            "Content-Type: application/json\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370210454798&s=zKjRgOMQ1Q3NB1kXqHJ6GPa6TlY=\";Pathr-Encoding: chunked\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "\n",
            "\n",
            "{\"FileStatus\":\n",
            "\n",
            "\n",
            "\n",
            "{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1370174432465,\"owner\":\"istvan\",\"pathSuffix\":\"\",\"permission\":\"7\n",
            "\n",
            "\n",
            "\n",
            "55\",\"replication\":0,\"type\":\"DIRECTORY\"}}\n",
            "\n",
            "This is similar to execute the Hadoop ls filesystem command:\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -ls /\n",
            "\n",
            "Warning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "Found 1 items\n",
            "\n",
            "drwxr-xr-x - istvan supergroup 0 2013-06-02 13:00 /tmp\n",
            "\n",
            "2./ Create a directory\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -X PUT \"http://localhost:50070/webhdfs/v1/tmp/webhdfs?user.name=istvan&op=MKDIRS\"\n",
            "\n",
            "curl -i \"http://<HOST>:<PORT>/webhdfs/v1/<PATH>?[user.name=<USER>&]op=...\"\n",
            "\n",
            "\n",
            "\n",
            "curl -i -X PUT \"http://172.19.36.142:50070/webhdfs/v1/user/hduser?op=CREATE [&overwrite=<true|false>][&blocksize=1523][&replication=1001] \n",
            "\n",
            "\n",
            "\n",
            "[&permission=CREATE][&buffersize=2001]\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "\n",
            "Content-Type: application/json\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370210530831&s=YGwbkw0xRVpEAgbZpX7wlo56RMI=\";Pathr-Encoding: chunked\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "The equivalent Hadoop filesystem command is as follows:\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -ls /tmp\n",
            "\n",
            "Warning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "Found 2 items\n",
            "\n",
            "drwxr-xr-x   - istvan supergroup          0 2013-06-02 12:17 /tmp/hadoop-istvan\n",
            "\n",
            "drwxr-xr-x   - istvan supergroup          0 2013-06-02 13:02 /tmp/webhdfs\n",
            "\n",
            "3./ Create a file\n",
            "\n",
            "\n",
            "\n",
            "To create a file requires two steps. First we need to run the command against the namenode then follows the redirection and execute the WebHDFS \n",
            "\n",
            "\n",
            "\n",
            "API against the appropriate datanode.\n",
            "\n",
            "\n",
            "\n",
            "Step 1:\n",
            "\n",
            "\n",
            "\n",
            "curl -i -X PUT \"http://localhost:50070/webhdfs/v1/tmp/webhdfs/webhdfs-test.txt?user.name=istvan&op=CREATE\"\n",
            "\n",
            "HTTP/1.1 307 TEMPORARY_REDIRECT\n",
            "\n",
            "Content-Type: application/octet-stream\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370210936666&s=BLAIjTpNwurdsgvFxNL3Zf4bzpg=\";Pathn: http://istvan-\n",
            "\n",
            "\n",
            "\n",
            "pc:50075/webhdfs/v1/tmp/webhdfs/webhdfs-test.txt?op=CREATE&user.name=istvan&overwrite=false\n",
            "\n",
            "Content-Length: 0\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "Step 2:\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -T webhdfs-test.txt \"http://istvan-pc:50075/webhdfs/v1/tmp/webhdfs/webhdfs-test.txt?op=CREATE&user.name=istvan&overwrite=false\"\n",
            "\n",
            "HTTP/1.1 100 Continue\n",
            "\n",
            "\n",
            "\n",
            "HTTP/1.1 201 Created\n",
            "\n",
            "Content-Type: application/octet-stream\n",
            "\n",
            "Location: webhdfs://0.0.0.0:50070/tmp/webhdfs/webhdfs-test.txt\n",
            "\n",
            "Content-Length: 0\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "To validate the result of the WebHDFS API we can run the following Hadoop filesystem command:\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -ls /tmp/webhdfs\n",
            "\n",
            "Warning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "Found 1 items\n",
            "\n",
            "-rw-r--r--   1 istvan supergroup         20 2013-06-02 13:09 /tmp/webhdfs/webhdfs-test.txt\n",
            "\n",
            "4./ Open and read a file\n",
            "\n",
            "\n",
            "\n",
            "In this case we run curl with -L option to follow the HTTP temporary redirect URL.\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -L \"http://localhost:50070/webhdfs/v1/tmp/webhdfs/webhdfs-test.txt?op=OPEN&user.name=istvan\"\n",
            "\n",
            "HTTP/1.1 307 TEMPORARY_REDIRECT\n",
            "\n",
            "Content-Type: application/octet-stream\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370211032526&s=suBorvpvTUs6z/sw5n5PiZWsUnU=\";Pathn: http://istvan-\n",
            "\n",
            "\n",
            "\n",
            "pc:50075/webhdfs/v1/tmp/webhdfs/webhdfs-test.txt?op=OPEN&user.name=istvan&offset=0\n",
            "\n",
            "Content-Length: 0\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "\n",
            "Content-Type: application/octet-stream\n",
            "\n",
            "Content-Length: 20\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "\n",
            "\n",
            "Hadoop WebHDFS test\n",
            "\n",
            "The corresponding Hadoop filesystem is as follows:\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -cat /tmp/webhdfs/webhdfs-test.txt\n",
            "\n",
            "Warning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "Hadoop WebHDFS test\n",
            "\n",
            "5./ Rename a directory\n",
            "\n",
            "READ DATA ;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "curl -i -L \"http://172.19.36.143:50070/webhdfs/v1/user/hduser/test?op=OPEN&user.name=hdfs\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -X PUT \"http://localhost:50070/webhdfs/v1/tmp/webhdfs?op=RENAME&user.name=istvan&destination=/tmp/webhdfs-new\"\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "\n",
            "Content-Type: application/json\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370211103159&s=Gq/EBWZTBaoMk0tkGoodV+gU6jc=\";Pathr-Encoding: chunked\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "To validate the result we can run the following Hadoop filesystem command:\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -ls /tmp\n",
            "\n",
            "Warning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "Found 2 items\n",
            "\n",
            "drwxr-xr-x   - istvan supergroup          0 2013-06-02 12:17 /tmp/hadoop-istvan\n",
            "\n",
            "drwxr-xr-x   - istvan supergroup          0 2013-06-02 13:09 /tmp/webhdfs-new\n",
            "\n",
            "6./ Delete a directory\n",
            "\n",
            "\n",
            "\n",
            "This scenario results in an exception if the directory is not empty since a non-empty directory cannot be deleted.\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -X DELETE \"http://localhost:50070/webhdfs/v1/tmp/webhdfs-new?op=DELETE&user.name=istvan\"\n",
            "\n",
            "HTTP/1.1 403 Forbidden\n",
            "\n",
            "Content-Type: application/json\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370211266383&s=QFIJMWsy61vygFExl91Sgg5ME/Q=\";Pathr-Encoding: chunked\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "\n",
            "\n",
            "{\"RemoteException\":{\"exception\":\"IOException\",\"javaClassName\":\"java.io.IOException\",\"message\":\"/tmp/webhdfs-new\n",
            "\n",
            "First the file in the directory needs to be deleted and then the empty directory can be deleted, too.\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -X DELETE \"http://localhost:50070/webhdfs/v1/tmp/webhdfs-new/webhdfs-test.txt?op=DELETE&user.name=istvan\"\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "\n",
            "Content-Type: application/json\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370211375617&s=cG6727hbqGkrk/GO4yNRiZw4QxQ=\";Pathr-Encoding: chunked\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -ls /tmp/webhdfs-newWarning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "$ curl -i -X DELETE \"http://localhost:50070/webhdfs/v1/tmp/webhdfs-new?op=DELETE&user.name=istvan&destination=/tmp/webhdfs-new\"\n",
            "\n",
            "HTTP/1.1 200 OK\n",
            "\n",
            "Content-Type: application/json\n",
            "\n",
            "Expires: Thu, 01-Jan-1970 00:00:00 GMT\n",
            "\n",
            "Set-Cookie: hadoop.auth=\"u=istvan&p=istvan&t=simple&e=1370211495893&s=hZcZFDOL0x7exEhn14RlMgF4a/c=\";Pathr-Encoding: chunked\n",
            "\n",
            "Server: Jetty(6.1.26)\n",
            "\n",
            "\n",
            "\n",
            "$ bin/hadoop fs -ls /tmpWarning: $HADOOP_HOME is deprecated.\n",
            "\n",
            "\n",
            "\n",
            "Found 1 items\n",
            "\n",
            "drwxr-xr-x   - istvan supergroup          0 2013-06-02 12:17 /tmp/hadoop-istvan\n",
            "\n",
            "\n",
            "\n",
            "H46907279\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*************kerberos*****************\n",
            "\n",
            "\n",
            "\n",
            " sudo yum install krb5-server krb5-libs krb5-auth-dialog krb5-workstation\n",
            "\n",
            " cat /etc/krb5.conf\n",
            "\n",
            "\n",
            "\n",
            " sudo nano krb5.conf\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[logging]\n",
            "\n",
            " default = FILE:/var/log/krb5libs.log\n",
            "\n",
            " kdc = FILE:/var/log/krb5kdc.log\n",
            "\n",
            " admin_server = FILE:/var/log/kadmind.log\n",
            "\n",
            "\n",
            "\n",
            "[libdefaults]\n",
            "\n",
            " default_realm = EXAMPLE.COM\n",
            "\n",
            " dns_lookup_realm = false\n",
            "\n",
            " dns_lookup_kdc = false\n",
            "\n",
            " ticket_lifetime = 24h\n",
            "\n",
            " renew_lifetime = 7d\n",
            "\n",
            " forwardable = true\n",
            "\n",
            "\n",
            "\n",
            "[realms]\n",
            "\n",
            " recon143.idfc.COM = {\n",
            "\n",
            " kdc = recon142.idfc.com\n",
            "\n",
            " admin_server = recon143.idfc.com\n",
            "\n",
            "\n",
            "\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "[domain_realm]\n",
            "\n",
            " .example.com = EXAMPLE.COM\n",
            "\n",
            " example.com = EXAMPLE.COM\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " KDC recon142.idfc.com\n",
            "\n",
            " Relam recon142.idfc.com\n",
            "\n",
            " domain\trecon143.idfc.com\n",
            "\n",
            " principal - admin/admin@recon143.com\n",
            "\n",
            " \n",
            "\n",
            "hdp cluster -KDC=recon142.com= KDC-HADOOP.com\n",
            "\n",
            "Windows= recon142.com=ADmin-AD.com\n",
            "\n",
            "\n",
            "\n",
            "/usr/sbin/kdb5_util create -s\n",
            "\n",
            "hortonworks\n",
            "\n",
            "hotronworks\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sudo /etc/rc.d/init.d/krb5kdc start\n",
            "\n",
            "sudo /etc/rc.d/init.d/kadmin start\n",
            "\n",
            "\n",
            "\n",
            "sudo /etc/rc.d/init.d/krb5kdc restart\n",
            "\n",
            "sudo /etc/rc.d/init.d/kadmin restart\n",
            "\n",
            "\n",
            "\n",
            "sudo yum install krb5-workstation\n",
            "\n",
            "kdc on recon142\n",
            "\n",
            "admin_server recon143\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "chkconfig krb5kdc on\n",
            "\n",
            "chkconfig kadmin on\n",
            "\n",
            "\n",
            "\n",
            "chkconfig krb5kdc on\n",
            "\n",
            "chkconfig kadmin on\n",
            "\n",
            "Create admin principal\n",
            "\n",
            "kadmin.local -q \"addprinc admin/admin\"\n",
            "\n",
            "Update kadm5.acl to make admin an administrator\n",
            "\n",
            "vi /var/kerberos/krb5kdc/kadm5.acl\n",
            "\n",
            "*/admin@HORTONWORKS.COM */recon143.idfc.com\n",
            "\n",
            "\n",
            "\n",
            "Restart kerberos services\n",
            "\n",
            "/etc/rc.d/init.d/krb5kdc restart\n",
            "\n",
            "/etc/rc.d/init.d/kadmin restart\n",
            "\n",
            "\n",
            "\n",
            "sudo service krb5kdc start\n",
            "\n",
            "sudo service kadmin start \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "KDC Type: Existing MIT KDC\n",
            "\n",
            "KDC Host: recon142.idfc.com\n",
            "\n",
            "Realm Name: recon142.idfc.com\n",
            "\n",
            "Executable path: /usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "giving permission\n",
            "\n",
            "hadoop fs -chmod -R 755 /user\n",
            "\n",
            " 644 for new files\n",
            "\n",
            " 755 - directories\n",
            "\n",
            " \n",
            "\n",
            "http://spryinc.com/blog/configuring-kerberos-security-hortonworks-data-platform-20\n",
            "\n",
            "https://github.com/abajwa-hw/security-workshops/blob/master/Setup-kerberos-Ambari.md\n",
            "\n",
            "https://major.io/2012/02/05/the-kerberos-haters-guide-to-installing-kerberos/\n",
            "\n",
            "\n",
            "\n",
            "hadoop fs -chmod 777 user\n",
            "\n",
            "\n",
            "\n",
            "8088\n",
            "\n",
            "50090\n",
            "\n",
            "50070-NM\n",
            "\n",
            "50075-DN\n",
            "\n",
            "H46907279\n",
            "\n",
            "\n",
            "\n",
            "google\n",
            "\n",
            "payment pay online -19-8-2015\n",
            "\n",
            "4726426894609470\n",
            "\n",
            "\n",
            "\n",
            "34879448\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "releam=\n",
            "\n",
            "\n",
            "\n",
            "**************LDAP SETup-------------\n",
            "\n",
            "\n",
            "\n",
            "http://docs.adaptivecomputing.com/viewpoint/hpc/Content/topics/1-setup/installSetup/settingUpOpenLDAPOnCentos6.htm\n",
            "\n",
            "https://www.centos.org/docs/5/html/Deployment_Guide-en-US/s1-ldap-quickstart.html\n",
            "\n",
            "\n",
            "\n",
            "LDAP key {SSHA}xZPAn6HTVCRlwJiqtDcuao6pNtnwWXIH\n",
            "\n",
            "\n",
            "\n",
            "ldapadd -f recon143.idfc.ldif -D cn=Manager,dc=recon143.idfc,dc=com -w hduser\n",
            "\n",
            "ldapsearch -x -LLL -b dc=recon143.idfc,dc=com\n",
            "\n",
            "\n",
            "\n",
            "add users group in ldap\n",
            "\n",
            "ldapadd -f users.ldif -D cn=Manager,dc=recon143.idfc,dc=com -w hduser\n",
            "\n",
            "add user to group\n",
            "\n",
            "ldapadd -f praful.ldif -D cn=Manager,dc=recon143.idfc,dc=com -w hduser\n",
            "\n",
            "\n",
            "\n",
            "assign group to user while creating a group\n",
            "\n",
            "ldapadd -f Recon.ldif -D cn=Manager,dc=recon143.idfc,dc=com -w hduser\n",
            "\n",
            "\n",
            "\n",
            "ldapadd -f addUserToRecon.ldif -D cn=Manager,dc=recon143.idfc,dc=com -w hduser\n",
            "\n",
            "\n",
            "\n",
            "locate cacerts | grep jre\n",
            "\n",
            "/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/security/cacerts\n",
            "\n",
            "\n",
            "\n",
            "export PATH=$PATH:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/bin\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "sudo yum purge krb5-admin-server krb5-auth-dialog krb5-clients krb5-config krb5-doc krb5-gss-samples krb5-kdc krb5-kdc-ldap krb5-locales krb5-\n",
            "\n",
            "\n",
            "\n",
            "multidev krb5-otp krb5-pkinit krb5-strength krb5-sync-plugin krb5-sync-tools krb5-telnetd krb5-user\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "http://hortonworks.com/blog/hadoop-groupmapping-ldap-integration/\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Recon id - FIC_CASH_APAC_19076-Done\n",
            "\n",
            "EGL_GL_KPLUS_08142015165131_IN,\n",
            "\n",
            "GL_RECON_14082015025612_kplus_Out\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Recon id - FIC_CASH_APAC_19084\n",
            "\n",
            "EGL_GL_TIPLUS_2207201508453693_IN, \n",
            "\n",
            "GL_RECON_14082015025612_TIPLUS_Out\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "Recon id - FIC_CASH_APAC_19074-74\n",
            "\n",
            "GL_RECON_22072015040006_CBS_OUT, \n",
            "\n",
            "EGL_GL_CBS_2207201508453693_IN\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "START THE RECON\n",
            "\n",
            "sudo service vsftpd start (143)\n",
            "\n",
            "cd $RBEAPP_HOME\n",
            "\n",
            "bin/recon_run.sh FIC_CASH_APAC_19076 14-AUG-2015\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hues installation  good path \n",
            "\n",
            "https://developer.ibm.com/hadoop/blog/2015/06/02/deploying-hue-on-ibm-biginsights/\n",
            "\n",
            "carrierpoint consultancy point\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "curl -i -L \"http://172.19.36.224:50070/webhdfs/v1/user/hduser/praful/praful.txt-test.txt?op=OPEN&user.name=hduser\"\n",
            "\n",
            "\n",
            "\n",
            "correct one to read \n",
            "\n",
            "\n",
            "\n",
            "sudo curl -i -L \"http://172.19.36.224:50070/webhdfs/v1/user/hduser/praful/praful.txt?op=OPEN&user.name=hduser\"\n",
            "\n",
            "\n",
            "\n",
            "iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT -m comment --comment \"Ambari Server\"\n",
            "\n",
            "\n",
            "\n",
            "service iptables save\n",
            "\n",
            "iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -s 192.168.1.0/24 -j ACCEPT -m comment --comment \"Tomcat Server port\"\n",
            "\n",
            "\n",
            "\n",
            "curl -L \"http://172.19.36.142:50070/webhdfs/v1/user/hduser/PRAFUL/PRAFUL.txt?op=OPEN&offset=0&length=30720&buffersize=1024\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "http://172.19.36.142:50070/webhdfs/v1/user/user2/PRAFUL.txt?op=OPEN&offset=0&length=30720&buffersize=1024\n",
            "\n",
            "\n",
            "\n",
            "sudo  curl -L \"http://172.19.36.142:50070/webhdfs/v1/user/user2/PRAFUL.txt?op=OPEN&offset=0&length=30720&buffersize=1024\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "http://172.19.36.142:50070/webhdfs/v1/user/user2/PRAFUL.txt?op=OPEN&offset=0&length=30720&buffersize=1024\n",
            "\n",
            "\n",
            "\n",
            "curl -L http://172.19.36.142:50070/webhdfs/v1/user/hduser/PRAFUL/PRAFUL.txt?op=OPEN&offset=0&length=30720&buffersize=1024\n",
            "\n",
            "\n",
            "\n",
            "curl  -L \"http://172.19.36.142:50070/webhdfs/v1/user/hduser/PRAFUL/PRAFUL.txt?op=OPEN&user.name=hduser\"\n",
            "\n",
            "sudo curl -i -L \"http://172.19.36.224:50070/webhdfs/v1/user/hduser/PRAFUL/PRAFUL.txt?op=OPEN&user.name=hduser\n",
            "\n",
            "\n",
            "\n",
            "sudo curl -i -L \"http://172.19.36.142:50070/webhdfs/v1/user/hduser/PRAFUL/PRAFUL.txt?op=OPEN\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "172.16.10.42 SMTP server Ip, Port 25 \n",
            "\n",
            "\n",
            "\n",
            "etc/yum/pluginconf.d/refresh-packagekit.\n",
            "\n",
            "\n",
            "\n",
            "yum install yum-plugin-priorities\n",
            "\n",
            "\n",
            "\n",
            "Sarath Chandra (Algofusiontech)Go to: /domain/jboss-as-7.1.1.Final\n",
            "\n",
            "run: bin/jboss-cli.sh --connect command=hutdown\n",
            "\n",
            "run bin/jboss-cli.sh --connect command=: shutdown\n",
            "\n",
            "\n",
            "\n",
            "18:31Sarath Chandra (Algofusiontech)go to: /domain/jboss-as-7.1.1.Final/standalone/deployments\n",
            "\n",
            "delete previous war and .deployed files\n",
            "\n",
            "copy new war file\n",
            "\n",
            "go back to joss installation folder\n",
            "\n",
            "run bin/standalone.sh -b=0.0.0.0 -bmanagement=0.0.0.0 &\n",
            "\n",
            "\n",
            "\n",
            "18:35Prafulla Dash (Algofusion)172.16.10.42 SMTP server Ip, Port 25 \n",
            "\n",
            "\n",
            "\n",
            "*************PRODUCTION************\n",
            "\n",
            "\n",
            "\n",
            "to know more about JAVA\n",
            "\n",
            "sudo update-alternatives --config java\n",
            "\n",
            " ls -l /usr/bin/java\n",
            "\n",
            "  ls -l /etc/alternatives/java\n",
            "\n",
            "sudo update-alternatives --config javac\n",
            "\n",
            " cd /usr/lib/jvm/java-1.7.0-openjdk.x86_64\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " /usr/lib/jvm/jre-1.7.0-openjdk.x86_64/bin/java\n",
            "\n",
            "\n",
            "\n",
            "1.7.0_09\n",
            "\n",
            "\n",
            "\n",
            "how to recognise java path\n",
            "\n",
            "\n",
            "\n",
            "/usr/lib/jvm/java-1.7.0-openjdk.x86_64\n",
            "\n",
            "1653\n",
            "\n",
            "idfcreconcluster\n",
            "\n",
            "\n",
            "\n",
            "Master\n",
            "\n",
            "\n",
            "\n",
            "DCRCNMDWPRR1.idfcbank.com\n",
            "\n",
            "DCRCNMDWPRR2.idfcbank.com\n",
            "\n",
            "DCRCNMDWPRR3.idfcbank.com\n",
            "\n",
            "DCRCNMDWPRR4.idfcbank.com\n",
            "\n",
            "\n",
            "\n",
            "hive\n",
            "\n",
            "hive\n",
            "\n",
            "hive\n",
            "\n",
            "10.1.37.101\n",
            "\n",
            "1653\n",
            "\n",
            "recondb\n",
            "\n",
            "jdbc:oracle:thin:@10.1.37.101:1653:recondb\n",
            "\n",
            "\n",
            "\n",
            "GL_RECON_22072015040006_CBS_OUT.txt\n",
            "\n",
            "\n",
            "\n",
            "sed -n \"1,10p\" /RECONUAT/Novopay/Novopay_Incremental_Transaction_15092015153436.txt | cat -n\n",
            "\n",
            "to see top 10 lines with numbering\n",
            "\n",
            "\n",
            "\n",
            "File: /user/hduser/AFTRP/recon_jobs/executions/bp/business_context_id=1111305315/recon_id=BP_CASH_APAC_18744/statement_date=2015-07-\n",
            "\n",
            "\n",
            "\n",
            "14/recon_execution_id=14974/source_id=2239/feed_id=2239_1747/stage=PREPROCESSED/CBS_DDFunding_recon_23072015.txt\n",
            "\n",
            "\n",
            "\n",
            "Big Data Now: 2012 Edition\n",
            "\n",
            "Python For Data Analysis\n",
            "\n",
            "Effective Awk Programming\n",
            "\n",
            "A Practical Guide To Linux Commands- Editors And Shell Programming\n",
            "\n",
            "Pro Apache Hadoop\n",
            "\n",
            "Hadoop Application Architectures\n",
            "\n",
            "PROGRAMMING PIG\n",
            "\n",
            "\n",
            "\n",
            "FIC_CASH_APAC_18904\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "TRE-004F10_22092015_1.txt\n",
            "\n",
            "TRE-004F10_22092015_0.txt-18925\n",
            "\n",
            "BGL----\n",
            "\n",
            "\n",
            "\n",
            "TRE-003F8_22092015_0.TXT-18904\n",
            "\n",
            "\n",
            "\n",
            "awk -F '|' '$10~\"UTR\" {print}' *OUT* to print some perticular values like UTR\n",
            "\n",
            "\n",
            "\n",
            "find new entry to perticular directory run from the the dir only\n",
            "\n",
            "find . -ctime  -1 -print >>\n",
            "\n",
            "\n",
            "\n",
            "FIC_CASH_APAC_18904\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "hadoop distcp\n",
            "\n",
            "\n",
            "\n",
            "FIC\n",
            "\n",
            "\n",
            "\n",
            "TRE-004F10_22092015_1.txt\n",
            "\n",
            "TRE-004F10_22092015_0.txt\n",
            "\n",
            "sony50w950c\n",
            "\n",
            "\n",
            "\n",
            "EGL_GL_BSMART_22072015050747_IN.txt\n",
            "\n",
            "EGL_GL_BSMART_22072015110722_OUT.txt\n",
            "\n",
            "\n",
            "\n",
            "EGL_GL_BSMART_22072015050747_IN.txt\n",
            "\n",
            "EGL_GL_BSMART_22072015110722_OUT.txt\n",
            "\n",
            "\n",
            "\n",
            "Linux Commands-\n",
            "\n",
            "\n",
            "\n",
            "cut -d : -f 1 /etc/passwd\t\t\tlist users \n",
            "\n",
            "less /etc/group\t\t\t\t\t\tsame as above\n",
            "\n",
            "more /etc/group\t\t\t\t\t\tsame as above\n",
            "\n",
            "cut -d : -f 1 /etc/group\t\t\tSame as above\n",
            "\n",
            "lsblk\t\t\t\t\t\t\t\tRAM\n",
            "\n",
            "lsblk -l\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(\"V-0002F3_<DATE_STRING>_(\\\\d{3}).txt\"))\n",
            "\n",
            "\n",
            "\n",
            "(\"V-0002F3_DDMMYYYY_(\\\\d{3}).txt\"))\n",
            "\n",
            "\n",
            "\n",
            "V-0002F3_<DDMMYYYY>_<seqnum 0 to 999>\n",
            "\n",
            "\n",
            "\n",
            "Vsmart_DisbPymtAcc_Recon.txt\n",
            "\n",
            "\n",
            "\n",
            "CBS_DisbPymtAcc_Recon.txt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(\"INRBGL_DDMMYYYY_(\\\\d{3}).txt\"))\n",
            "\n",
            "\n",
            "\n",
            "INRBGL_<DDMMYYYY>_<seqnum 0 to 999>\n",
            "\n",
            "\n",
            "\n",
            "(\"V-0001F1_DDMMYYYY_26092015040930.txt(\\\\d{3}).txt\"))\n",
            "\n",
            "\n",
            "\n",
            "V-0001F1_26092015040930.txt\n",
            "\n",
            "\n",
            "\n",
            "natural ac = TEXT\n",
            "\n",
            "DE151\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "18724\n",
            "\n",
            "\n",
            "\n",
            "  Production Movement\n",
            "\n",
            "\n",
            "\n",
            "a.       The same config base, application base tested in UAT2.0 on Wednesday would be moved to Production environment , point UAT 2.0  MFT and \n",
            "\n",
            "\n",
            "\n",
            "perform the \n",
            "\n",
            "\n",
            "\n",
            "Dry runs on daily basis. Responsibility lies with Jilani, Prafulla and Poornima - My understanding is that when we move new recon to upper \n",
            "\n",
            "\n",
            "\n",
            "environments we \n",
            "\n",
            "\n",
            "\n",
            "should be doing dry run and test it. But doing daily basis in UAT should not be developing team's task. Let me know if difference in opinion.\n",
            "\n",
            "\n",
            "\n",
            "b.      Sarath , Roopesh and offshore team provides the necessary support as needed and resolve the issues. - Fine\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "EGL_GL_KPLUS_09262015[0-9]_IN.txt\n",
            "\n",
            "TRIALBAL_26092015_[0-9]{6}.txt\n",
            "\n",
            "TRSC_CASH_APAC_18904\n",
            "\n",
            "223\n",
            "\n",
            "reconuat\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "/home/reconuat/26092015\n",
            "\n",
            "\n",
            "\n",
            "CMS_IDFCDD_#yyyyMMdd#[0-9]{6}.txt\n",
            "\n",
            "BGL-OUTFILE_#ddMMyyyy#_[0-9]{6}.txt\n",
            "\n",
            "BGL-OUTFILE_#ddMMyyyy#_[0-9]{6}.txt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Use 18646 as template\n",
            "\n",
            "BGL & Tran - #ACCOUNT_NUMBER# ~ /$FEED_ACCOUNTS$/ &amp;&amp; $1==\"D\"\n",
            "\n",
            "BGL - Pos - BGL & Tran - #ACCOUNT_NUMBER# ~ /$FEED_ACCOUNTS$/ &amp;&amp; $1==\"H\"\n",
            "\n",
            "\n",
            "\n",
            "Txn based\n",
            "\n",
            "<SplitRequired>true</SplitRequired>\n",
            "\n",
            "<SplitCondition>$FEED_ACCOUNTS$ &amp;&amp; #SOURCE#==\"D\"</SplitCondition>\n",
            "\n",
            "<SplitFileSuffix>18583_T</SplitFileSuffix>\n",
            "\n",
            "\n",
            "\n",
            "position based\n",
            "\n",
            "<SplitRequired>true</SplitRequired>\n",
            "\n",
            "<SplitCondition>#ACCOUNT_NUMBER# ~ /$FEED_ACCOUNTS$/ &amp;&amp; #SOURCE#==\"H\"</SplitCondition>\n",
            "\n",
            "<SplitFileSuffix>18583_P</SplitFileSuffix>\n",
            "\n",
            "\n",
            "\n",
            "source based\n",
            "\n",
            "<SplitRequired>true</SplitRequired>\n",
            "\n",
            "<SplitCondition>#SOURCE# == \"KPLUS\"</SplitCondition>\n",
            "\n",
            "<SplitFileSuffix>KPLUS</SplitFileSuffix>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Vertical spliting\n",
            "\n",
            "\n",
            "\n",
            "<SplitRequired>true</SplitRequired>\n",
            "\n",
            "<SplitScriptFile>./vsplit.sh</SplitScriptFile>\n",
            "\n",
            "<SplitFileSuffix>2623_2113</SplitFileSuffix>\n",
            "\n",
            "   \n",
            "\n",
            "  \n",
            "\n",
            " only account like copy--VSOFT\n",
            "\n",
            " \n",
            "\n",
            "<SplitRequired>true</SplitRequired>\n",
            "\n",
            "<SplitCondition>#ACCOUNT_NUMBER#!=\"\" </SplitCondition>\n",
            "\n",
            "<SplitFileSuffix>3124_3124</SplitFileSuffix>\n",
            "\n",
            "\n",
            "\n",
            "Remove 2 lines from top- NPCI\n",
            "\n",
            "\n",
            "\n",
            "<SplitRequired>true</SplitRequired>\n",
            "\n",
            "<SplitCondition>NR>3</SplitCondition>\n",
            "\n",
            "<SplitFileSuffix>3124_3124</SplitFileSuffix>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "oozie issue\n",
            "\n",
            "/var/tmp/oozie\n",
            "\n",
            "su oozie\n",
            "\n",
            "/usr/lib/oozie/bin/ooziedb.sh create -sqlfile oozie.sql -run\n",
            "\n",
            "\t\t\t\t\t\t\n",
            "\n",
            "hadoop fs -chown -R reconadmin:hdfs /user/hduser\n",
            "\n",
            "\n",
            "\n",
            "sudo chown -R reconadmin:reconadmin /user/local/hadoop-2.7.1\n",
            "\n",
            "\n",
            "\n",
            "DCRCNAPPPRR1\n",
            "\n",
            "\n",
            "\n",
            "reconadmin\n",
            "\n",
            "\n",
            "\n",
            "/recondata/VSMART\n",
            "\n",
            "\n",
            "\n",
            "ABORT_JOB\n",
            "\n",
            "\n",
            "\n",
            "9008290547-4g no\n",
            "\n",
            "FOR OGL\n",
            "\n",
            "/recondata/EGL\n",
            "\n",
            "\n",
            "\n",
            "/recondata/CBS\n",
            "\n",
            "/recondata/CMS\n",
            "\n",
            "\n",
            "\n",
            "/user/reconadmin/algorecon/recon_jobs/1111148147112/FIC_CASH_APAC_18663/conf/\n",
            "\n",
            "I want to return my product as it is not feet for my phone. Please send some one to pick it up.\n",
            "\n",
            "WBZ100\n",
            "\n",
            "\n",
            "\n",
            "<configuration>\n",
            "\n",
            "\n",
            "\n",
            "<property>\n",
            "\n",
            "    <name>dfs.namenode.http-address</name>\n",
            "\n",
            "    <value>localhost:50070</value>\n",
            "\n",
            "</property>\n",
            "\n",
            "\n",
            "\n",
            "<property>\n",
            "\n",
            "    <name>dfs.datanode.http.address</name>\n",
            "\n",
            "    <value>localhost:50075</value>\n",
            "\n",
            "</property>\n",
            "\n",
            "\n",
            "\n",
            "  <property>\n",
            "\n",
            "    <name>dfs.replication</name>\n",
            "\n",
            "    <value>2</value>\n",
            "\n",
            "  </property>\n",
            "\n",
            "  <property>\n",
            "\n",
            "    <name>dfs.namenode.name.dir</name>\n",
            "\n",
            "    <value>/home/hduser/hdfs_data/nn</value>\n",
            "\n",
            "  </property>\n",
            "\n",
            "  <property>\n",
            "\n",
            "    <name>dfs.datanode.data.dir</name>\n",
            "\n",
            "    <value>/home/hduser/hdfs_data/dn</value>\n",
            "\n",
            "  </property>\n",
            "\n",
            "</configuration>\n",
            "\n",
            "GRAB4\n",
            "\n",
            "BILL50\n",
            "\n",
            "9008290547--4g\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3e50e1b1a067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"awk '!x[$0]++' file.txt > file4.txt\")\n",
        "os.remove('file2.txt')"
      ],
      "metadata": {
        "id": "_O_krmKJ50Ua"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}